{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sklearn\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1,\"/home/sbulusu/qcd_ml/neural_networks/libs/\")\n",
    "\n",
    "import dataset_class_gsimage\n",
    "import aenc_torch_net_class\n",
    "#import aenc_utils\n",
    "import utils\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizing CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Utilizing CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizing CPU\")\n",
    "    \n",
    "cpu_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom torchvision import datasets, transforms\\n# MNIST Dataset\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize(mean=[0.5], std=[0.5])])\\n\\ntrain_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\\ntest_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\\n\\n# Data Loader (Input Pipeline)\\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Download dataset provided by pytorch\"\"\"\n",
    "\"\"\"\n",
    "from torchvision import datasets, transforms\n",
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-334982a91ba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#print(dataset.get_image(i)[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "n_load = 10\n",
    "i = 0\n",
    "\n",
    "fig, ax = plt.subplots(1,n_load)\n",
    "for i in range(n_load):\n",
    "    ax[i].imshow(train_dataset[i][0].detach().cpu().numpy().reshape([28,28]))\n",
    "#plt.show()\n",
    "#print(dataset.get_image(i)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"...But we will be using our custom dataset and functions!\"\"\"\n",
    "\n",
    "#mnist_image_folder = \"../data/mnist/\"\n",
    "#folder of the dataset\n",
    "mnist_image_folder = \"/media/data/sbulusu/datasets/mnist/mnist_images/\"\n",
    "mnist_image_names = os.listdir(mnist_image_folder)\n",
    "#relative paths of all images to mnist_image_folder\n",
    "mnist_image_paths = glob.glob(\"mnist/*\")\n",
    "image_size = [1,28,28]\n",
    "#label_path_csv = \"../data/mnist_labels.csv\"\n",
    "label_path_csv = \"/media/data/sbulusu/datasets/mnist/mnist_labels.csv\"\n",
    "dataset = dataset_class_gsimage.image_dataset(mnist_image_folder, mnist_image_paths, image_size, label_path_csv, transform=\"default\", device=device)\n",
    "#dataset = dataset_class_gsimage.image_dataset(mnist_image_folder, mnist_image_paths, image_size, label_path_csv, transform=None, device=device)\n",
    "#name of the attribute in the csv file\n",
    "target_attributes = [\"number\"]\n",
    "dataset.set_label_names(target_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "                dataset_class_gsimage.send_to_device(target_device=cpu_device),\n",
    "                #dataset_class_gsimage.numpy(typ=np.uint8),\n",
    "                dataset_class_gsimage.torch_tensor(typ=torch.uint8),\n",
    "                #dataset_class_gsimage.roll_channel_axis(old_pos=0, new_pos=-1, device=torch.device(\"cpu\")),\n",
    "                torchvision.transforms.ToPILImage(mode=None),\n",
    "                torchvision.transforms.RandomResizedCrop(size=(28,28),scale=(0.8, 1.0), ratio=(0.75, 1.33333), interpolation=2),\n",
    "                torchvision.transforms.RandomRotation((-30,+30), resample=False, expand=False, center=None),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                dataset_class_gsimage.send_to_device(target_device=device),\n",
    "                dataset_class_gsimage.min_max_scaler(device=device),\n",
    "                torchvision.transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "                torchvision.transforms.RandomApply([dataset_class_gsimage.add_normal_noise(0.,0.1,device=device)], p=0.5),\n",
    "                torchvision.transforms.RandomErasing(p=0.5, scale=(0.01, 0.05), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "n_load = 10\n",
    "i = 0\n",
    "\n",
    "fig, ax = plt.subplots(1,n_load)\n",
    "for i in range(n_load):\n",
    "    ax[i].imshow(dataset.get_image(i)[0].detach().cpu().numpy().reshape([28,28]))\n",
    "#plt.show()\n",
    "#print(dataset.get_image(i)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "                dataset_class_gsimage.min_max_scaler(device=device),\n",
    "                torchvision.transforms.Normalize(mean=[0.0], std=[1.0]),\n",
    "                #torchvision.transforms.RandomApply([dataset_class_gsimage.add_normal_noise(0.,0.1,device=device)], p=0.5),\n",
    "                #torchvision.transforms.RandomErasing(p=0.5, scale=(0.01, 0.05), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Manually set network structure\"\"\"\n",
    "\"\"\"\n",
    "    This list can be loaded into the constructor of the Net neural network class, to automatically generate the network structure\n",
    "    type = pointer to the layer function'\n",
    "    layer_pars = parameters which must be given to the layer function in order to initialize it\n",
    "    act_func = activation function to be applied directly after feeding to the corresponding layer\n",
    "    dropout = certain neurons cna be dropped out if specified\n",
    "\"\"\"\n",
    "\n",
    "input_size = dataset.get_input_size()\n",
    "output_size = input_size\n",
    "\n",
    "latent_size = 10\n",
    "\n",
    "fc_input_size = np.product(input_size)\n",
    "print(fc_input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_func = torch.relu\n",
    "#act_func = F.leaky_relu\n",
    "#act_func = torch.sigmoid\n",
    "\n",
    "encoder_struct = []\n",
    "encoder_struct.append( {\"type\": nn.Flatten, \"layer_pars\": {\"start_dim\": 1}} )\n",
    "encoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": fc_input_size, \"out_features\": fc_input_size//4}, \"bias\": True, \"act_func\": act_func} )\n",
    "encoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": fc_input_size//4, \"out_features\": fc_input_size//8}, \"bias\": True, \"act_func\": act_func} )\n",
    "encoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": fc_input_size//8, \"out_features\": fc_input_size//16}, \"bias\": True, \"act_func\": act_func} )\n",
    "encoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": fc_input_size//16, \"out_features\": latent_size}, \"bias\": True, \"act_func\": act_func} )\n",
    "\n",
    "encoder_sizes = utils.calc_layer_sizes(input_size, encoder_struct)\n",
    "print(encoder_sizes)\n",
    "\n",
    "latent_size = encoder_sizes[-1]\n",
    "input_size = encoder_sizes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_func = torch.relu\n",
    "#act_func = F.leaky_relu\n",
    "#act_func = torch.sigmoid\n",
    "\n",
    "decoder_struct = []\n",
    "\n",
    "decoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": latent_size, \"out_features\": fc_input_size//16}, \"bias\": True, \"act_func\": act_func} )\n",
    "decoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": fc_input_size//16, \"out_features\": fc_input_size//8}, \"bias\": True, \"act_func\": act_func} )\n",
    "decoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": fc_input_size//8, \"out_features\": fc_input_size//4}, \"bias\": True, \"act_func\": act_func} )\n",
    "decoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": fc_input_size//4, \"out_features\": fc_input_size}, \"bias\": True, \"act_func\": torch.sigmoid} )\n",
    "decoder_struct.append( {\"type\": utils.Reshape, \"layer_pars\": {\"new_shape\": input_size}} )\n",
    "#fixed_net_struct.append( {\"type\": nn.Softmax, \"layer_pars\": {\"dim\": 1}} )\n",
    "#dim 0 or 1???\n",
    "#fixed_net_struct.append( {\"type\": nn.ConvTranspose2d, \"layer_pars\": {\"in_channels\": 1, \"out_channels\": 1, \"kernel_size\": [1,1], \"stride\": 1, \"padding\": 1, \"bias\": True}, \"act_func\": act_func} )\n",
    "\n",
    "decoder_sizes = utils.calc_layer_sizes(latent_size, decoder_struct)\n",
    "\n",
    "print(decoder_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HYPERPARAMETERS\n",
    "\"\"\"\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "#val_epochs = [10,20,30,40,50,60,70]\n",
    "#val_epochs = [1,2]\n",
    "#val_epochs = [1,2,3,4,5,6,7,8,9,10]\n",
    "val_epochs = list(range(0,100))\n",
    "\n",
    "#save states of the network for particular epochs, sate can be reloaded afterwards!\n",
    "save_state_epochs = [10000]\n",
    "\n",
    "\n",
    "\"\"\"create list of parameters manually\"\"\"\n",
    "\n",
    "hyper_parameters = {}\n",
    "\n",
    "loss_func_kwargs = {\"reduction\": \"sum\"}\n",
    "#hyper_parameters[\"loss_func\"] = nn.CrossEntropyLoss\n",
    "#hyper_parameters[\"loss_func\"] = nn.BCELoss\n",
    "hyper_parameters[\"loss_func\"] = nn.MSELoss\n",
    "hyper_parameters[\"optimizer\"] = optim.Adam\n",
    "hyper_parameters[\"batch_size\"] = 20 \n",
    "hyper_parameters[\"lr\"] = 0.00001\n",
    "\n",
    "hyper_parameters[\"val_method\"] = \"holdout\"\n",
    "hyper_parameters[\"val_method_pars\"] = {\"train\" : 0.9, \"val\" : 0.1, \"test\" : 0.}\n",
    "#hyper_parameters[\"val_method\"] = \"k_fold\"\n",
    "#hyper_parameters[\"val_method_pars\"] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = max(val_epochs)+1\n",
    "\n",
    "lr=hyper_parameters[\"lr\"]\n",
    "batch_size = hyper_parameters[\"batch_size\"]\n",
    "loss_func = hyper_parameters[\"loss_func\"](**loss_func_kwargs)\n",
    "#loss_func = hyper_parameters[\"loss_func\"]()\n",
    "val_method = hyper_parameters[\"val_method\"]\n",
    "val_method_pars = hyper_parameters[\"val_method_pars\"]\n",
    "optimizer_type = hyper_parameters[\"optimizer\"]\n",
    "\n",
    "val_pred_paths = []\n",
    "val_label_paths = []\n",
    "\n",
    "train_loss = np.zeros(epochs)\n",
    "val_loss = np.zeros(len(val_epochs))\n",
    "\n",
    "net_state_paths = []\n",
    "\n",
    "#create training log\n",
    "log_file_name = \"log.txt\"\n",
    "log_file = open(log_file_name, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate network class instance\"\"\"\n",
    "net = aenc_torch_net_class.AutoEncoder(encoder_struct=encoder_struct, decoder_struct=decoder_struct, input_size=input_size, latent_size=latent_size, device=device)\n",
    "net.set_batch_size(batch_size)\n",
    "net.to(device)\n",
    "net_parameters = net.parameters()\n",
    "\n",
    "optimizer = optimizer_type(net_parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load and split data\"\"\"\n",
    "split_indices = utils.load_split_indices(dataset=dataset, batch_size=batch_size, method=val_method, method_pars=val_method_pars, shuffle=True, random_seed=random_seed, log_file=log_file)\n",
    "\n",
    "train_indices = split_indices[0][0]\n",
    "val_indices = split_indices[0][1]\n",
    "test_indices = split_indices[0][2]\n",
    "\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
    "val_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load data provided by pytorch\"\"\"\n",
    "\"\"\"\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dir = \"val/\"\n",
    "try:\n",
    "    os.makedirs(val_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "train_state_dir = \"net_states/\"\n",
    "try:\n",
    "    os.makedirs(train_state_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "epoch = 0\n",
    "val_i = 0\n",
    "train_start_time = time.time()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    batch_nr = 0\n",
    "    epoch_loss = np.zeros(len(train_loader))\n",
    "    \n",
    "    \"\"\"Actual training step\"\"\"\n",
    "    for train_mini_batch in tqdm(train_loader):\n",
    "        \n",
    "        batch_loss, train_output = utils.step(net, train_mini_batch[0], train_mini_batch[0], loss_func, optimizer, device, mode=\"train\", log_file=log_file)\n",
    "        epoch_loss[batch_nr] = batch_loss.item()\n",
    "        batch_nr += 1\n",
    "    \n",
    "    mean_epoch_loss = epoch_loss.mean()\n",
    "    train_loss[epoch] = mean_epoch_loss\n",
    "    print(f\"mean epoch {epoch} train loss: {mean_epoch_loss}\\n\")\n",
    "    \n",
    "    \n",
    "    \"\"\"save the neural networks state\"\"\"\n",
    "    if epoch in save_state_epochs:\n",
    "        train_state_epoch_file_path = train_state_dir + f\"state_epoch_{epoch}\"\n",
    "        #train_state = {\"epoch\" : epoch, \"state_dict\": net.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "        #torch.save(train_state, train_state_epoch_file_path)\n",
    "        torch.save(net.state_dict(), train_state_epoch_file_path)\n",
    "        net_state_paths.append(train_state_epoch_file_path)\n",
    "        print(f\"saved model from epoch {epoch}\")\n",
    "        \n",
    "    \"\"\"\n",
    "    Validation\n",
    "    \"\"\"\n",
    "    #val_epochs = [100000]\n",
    "    if epoch in val_epochs:\n",
    "        val_label = []\n",
    "        val_pred = []\n",
    "        \n",
    "        val_batch_nr = 0\n",
    "        val_epoch_loss = np.zeros(len(val_loader))\n",
    "        \n",
    "        for val_mini_batch in val_loader:\n",
    "            label_batch = val_mini_batch[1]\n",
    "            val_label.append(label_batch.detach().cpu().numpy())\n",
    "            val_batch_loss, val_output = utils.step(net, val_mini_batch[0], val_mini_batch[0], loss_func, optimizer, device, mode=\"val\", log_file=log_file)\n",
    "            val_epoch_loss[val_batch_nr] = val_batch_loss.item()\n",
    "            val_pred.append(val_output.detach().cpu().numpy())\n",
    "            \n",
    "            if val_batch_nr == 0:\n",
    "                num_images = 10\n",
    "                fig, ax = plt.subplots(1,num_images)\n",
    "                for i in range(num_images):\n",
    "                    ax[i].imshow(val_pred[-1].reshape((batch_size,1,28,28))[i,0])\n",
    "                plt.show()\n",
    "                #ax[1].imshow(val_label[0].reshape((batch_size,1,28,28))[0,0])\n",
    "            \n",
    "            val_batch_nr += 1\n",
    "            \n",
    "        mean_val_epoch_loss = val_epoch_loss.mean()\n",
    "        val_loss[val_i] = mean_val_epoch_loss\n",
    "        print(f\"mean epoch {epoch} val loss: {mean_val_epoch_loss}\\n\")\n",
    "        \n",
    "        val_i += 1\n",
    "        \n",
    "        print(f\"Saving validation results for epoch {epoch}\")\n",
    "        \n",
    "        val_pred_path = val_dir + \"/\" + f\"val_epoch_{epoch}_pred\"\n",
    "        val_label_path = val_dir + \"/\" + f\"val_epoch_{epoch}_labels\"\n",
    "        #print(np.array(functools.reduce(operator.iconcat, val_pred, [])))\n",
    "        np.array(functools.reduce(operator.iconcat, val_pred, [])).tofile(val_pred_path, sep=\" \")\n",
    "        np.array(functools.reduce(operator.iconcat, val_label, [])).tofile(val_label_path, sep=\" \")\n",
    "        #np.array(val_pred).tofile(val_pred_path)\n",
    "        #np.array(val_label).tofile(val_pred_label)\n",
    "        \n",
    "        val_pred_paths.append(val_pred_path)\n",
    "        val_label_paths.append(val_label_path)\n",
    "        print(f\"validation prediction: {val_pred_path}\")\n",
    "        print(f\"validation label: {val_label_path}\")\n",
    "        \n",
    "log_file.close()\n",
    "train_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training time: {train_end_time-train_start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot training loss curve and save as image\"\"\"\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "loss_img_file_name = \"train_loss.png\"\n",
    "x_epochs = range(epochs)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_epochs, train_loss, label=\"train loss\")\n",
    "ax.plot(val_epochs, val_loss, label=\"val loss\")\n",
    "leg = ax.legend()\n",
    "#plt.title()\n",
    "plt.xlabel(\"epoch\")\n",
    "#plt.ylabel(\"loss\")\n",
    "plt.savefig(loss_img_file_name)\n",
    "plt.show()\n",
    "plt.close()\n",
    "train_loss_txt_file_name = \"train_loss.txt\"\n",
    "np.savetxt(train_loss_txt_file_name, train_loss)\n",
    "val_loss_txt_file_name = \"val_loss.txt\"\n",
    "np.savetxt(val_loss_txt_file_name, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state_file_path = train_state_dir + f\"state_epoch_{epoch}\"\n",
    "#train_state = {\"epoch\" : epoch, \"state_dict\": net.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "torch.save(net.state_dict(), train_state_file_path)\n",
    "net_state_paths.append(train_state_file_path)\n",
    "print(f\"saved model from epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_epoch = epoch\n",
    "print(f\"loading model from epoch {load_epoch}\")\n",
    "net_state_path = f\"net_states/state_epoch_{load_epoch}\"\n",
    "net.load_state_dict(torch.load(net_state_path))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "test_image, test_label = dataset[0]\n",
    "indices = np.arange(0,10)\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=test_sampler)\n",
    "test_images = []\n",
    "outputs = []\n",
    "for test_batch in train_loader:\n",
    "    test_image = test_batch[0].to(device)\n",
    "    test_images.append(test_image)\n",
    "    #plt.imshow(test_image.cpu()[0,0])\n",
    "    net.eval()\n",
    "    output = net.forward(test_image.float())\n",
    "    outputs.append(output)\n",
    "\n",
    "num_images = len(outputs)\n",
    "fig, ax = plt.subplots(2,num_images)\n",
    "for image_i in range(num_images):\n",
    "    ax[0, image_i].imshow(test_images[image_i].cpu()[0,0])\n",
    "    ax[1, image_i].imshow(outputs[image_i].detach().cpu()[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "num_samples = 10\n",
    "#latent_samples = torch.randint(low=1, high=1000, size=(num_samples, latent_size) )\n",
    "latent_samples = torch.randn(size=(num_samples, latent_size) )\n",
    "\n",
    "output = net.decoder(latent_samples.to(device).float())\n",
    "print(output.size())\n",
    "print(len(output))\n",
    "fig, ax = plt.subplots(1,num_samples)\n",
    "for i in range(num_samples):\n",
    "    ax[i].imshow(output.detach().cpu()[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(dataset.label_df[\"number\"], dtype=int)\n",
    "sorted_label_indices = []\n",
    "for class_num in range(0,10):\n",
    "    sorted_label_indices.append(np.array(np.where(labels == class_num))[0])\n",
    "    print(np.array(np.where(labels == class_num)).shape)\n",
    "    \n",
    "#print(len(sorted_label_indices))\n",
    "#print(sorted_label_indices[0].size)\n",
    "#print(sorted_label_indices[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vectors = []\n",
    "\n",
    "for class_num in range(0,10):\n",
    "    num_per_class = 100\n",
    "    #num_per_class = len(sorted_label_indices[class_num]\n",
    "    \n",
    "    class_latent_vectors = np.zeros( (num_per_class,latent_size) )\n",
    "\n",
    "    \n",
    "    #for image_i in range(len(sorted_label_indices[class_num])):\n",
    "    for image_i in tqdm(range(num_per_class)):\n",
    "        #print(image_i)\n",
    "        #get one image\n",
    "        image_index = sorted_label_indices[class_num][image_i]\n",
    "        #print(image_index)\n",
    "        image = dataset.get_image(image_index)[0]\n",
    "        if image_i < 3:\n",
    "            plt.imshow(image.detach().cpu()[0])\n",
    "            plt.show()\n",
    "        \n",
    "        #print(image.shape)\n",
    "        \n",
    "        #compress to latent space\n",
    "        class_latent_vectors[image_i] = net.encoder(image.float()).detach().cpu()\n",
    "        \n",
    "    \n",
    "    latent_vectors.append(class_latent_vectors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vectors = np.array(latent_vectors)\n",
    "print(latent_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_latent_vectors = latent_vectors.mean(axis=1)\n",
    "std_latent_vectors = latent_vectors.std(axis=1)\n",
    "\n",
    "print(\"mean latent vectors\")\n",
    "for i in range(0,10):\n",
    "    print(f\"number {i}\")\n",
    "    print(mean_latent_vectors[i])\n",
    "    print(\"std\")\n",
    "    print(std_latent_vectors[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "#gen_num = 2\n",
    "\n",
    "ex_per_num = 10\n",
    "\n",
    "fig, ax = plt.subplots(10,ex_per_num)\n",
    "\n",
    "for gen_num in range(0,10):\n",
    "\n",
    "    latent_shape = mean_latent_vectors[gen_num].shape\n",
    "    ex_shape = np.concatenate(([ex_per_num], latent_shape))\n",
    "    \n",
    "    latent_noise = np.random.normal(size=ex_shape)\n",
    "    #print(noise)\n",
    "    #sample_latent_vector = mean_latent_vectors[gen_num] + latent_noise\n",
    "    sample_latent_vector = np.zeros(ex_shape)\n",
    "    for ex in range(len(sample_latent_vector)):\n",
    "        sample_latent_vector[ex,:] = mean_latent_vectors[gen_num] + latent_noise[ex,:]\n",
    "        \n",
    "    ex_sample_images = net.decoder(torch.tensor(sample_latent_vector).to(device).float())\n",
    "    \n",
    "    for ex in range(len(sample_latent_vector)):\n",
    "        ax[gen_num, ex].imshow(ex_sample_images[ex,:,:].detach().cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "start = 5\n",
    "\n",
    "mean_interact_kwargs = {}\n",
    "for i in range(0,10):\n",
    "    key = f\"x{i}\"\n",
    "    #print(mean_latent_vectors.shape)\n",
    "    mu = mean_latent_vectors[start][i]\n",
    "    delta = 3.\n",
    "    low = mu - delta\n",
    "    high = mu + delta\n",
    "    mean_interact_kwargs[key] = (low,high)\n",
    "\n",
    "mean_interact_kwargs\n",
    "\n",
    "sample_i = 0\n",
    "\n",
    "sample_interact_kwargs = {}\n",
    "\n",
    "for i in range(0,10):\n",
    "    key = f\"x{i}\"\n",
    "    print(latent_vectors.shape)\n",
    "    mu = latent_vectors[start][sample_i][i]\n",
    "    delta = 6.\n",
    "    low = mu - delta\n",
    "    high = mu + delta\n",
    "    sample_interact_kwargs[key] = (low,high)\n",
    "    \n",
    "sample_interact_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vector = torch.zeros((latent_size)).to(device)\n",
    "\n",
    "#@interact(x0=(-5.,5.), x1=(-5.,5.), x2=(-5.,5.), x3=(-5.,5.), x4=(-5.,5.), x5=(-5.,5.), x6=(-5.,5.), x7=(-5.,5.), x8=(-5.,5.), x9=(-5.,5.))\n",
    "@interact(**sample_interact_kwargs)\n",
    "#@interact(**mean_interact_kwargs)\n",
    "#def update_latent_vector(x0=0., x1=0., x2=0., x3=0., x4=0., x5=0., x6=0., x7=0., x8=0., x9=0.):\n",
    "def update_latent_vector(x0, x1, x2, x3, x4, x5, x6, x7, x8, x9):\n",
    "    \n",
    "    latent_vector[0] = x0\n",
    "    latent_vector[1] = x1\n",
    "    latent_vector[2] = x2\n",
    "    latent_vector[3] = x3\n",
    "    latent_vector[4] = x4\n",
    "    latent_vector[5] = x5\n",
    "    latent_vector[6] = x6\n",
    "    latent_vector[7] = x7\n",
    "    latent_vector[8] = x8\n",
    "    latent_vector[9] = x9\n",
    "    print(latent_vector)\n",
    "    image = net.decoder(latent_vector).detach().cpu().numpy()[0][0]\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean latent vectors\")\n",
    "for i in range(0,10):\n",
    "    print(f\"number {i}\")\n",
    "    print(mean_latent_vectors[i])\n",
    "    #print(\"std\")\n",
    "    #print(std_latent_vectors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def update_latent_vector(dummy):\n",
    "    #index = 0\n",
    "    latent_vector = torch.zeros((1,latent_size)).to(device)\n",
    "    for i in range(0,latent_size):\n",
    "        latent_vector[0,i] = sliders[i].value\n",
    "        print(sliders[i].value)\n",
    "    print(latent_vector)\n",
    "    image = net.decoder(latent_vector.float()).detach().cpu().numpy()[0][0]\n",
    "    #print(image.shape)\n",
    "    #clear_output\n",
    "    plt.clf()\n",
    "    #plt.close()\n",
    "    plt.imshow(image)\n",
    "    #clear_output\n",
    "\n",
    "\n",
    "sliders = []\n",
    "for i in range(0,latent_size):\n",
    "    sliders.append(widgets.FloatSlider(min=-2., max=2.))\n",
    "    display(sliders[i])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range(0,latent_size):\n",
    "    sliders[i].observe(update_latent_vector, names=[\"value\"])\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Denoising images\"\"\"\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "test_image, test_label = dataset[0]\n",
    "indices = np.arange(0,15)\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices)\n",
    "noise_scale = 0.2\n",
    "noise_func = torch.randn\n",
    "noise_kwargs = {}\n",
    "\n",
    "\"\"\"\n",
    "dataset.transform = torchvision.transforms.Compose([\n",
    "                #torchvision.transforms.ToTensor(),\n",
    "                dataset_class_gsimage.min_max_scaler(device=device),\n",
    "                dataset_class_gsimage.add_normal_noise(0.,0.1,device=device),\n",
    "            ])\n",
    "\"\"\"\n",
    "\n",
    "dataset.transform = torchvision.transforms.Compose([\n",
    "                dataset_class_gsimage.min_max_scaler(device=device),\n",
    "            ])\n",
    "\n",
    "#dataset.transform = None\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=test_sampler)\n",
    "test_images = []\n",
    "noisy_images = []\n",
    "outputs = []\n",
    "for test_batch in train_loader:\n",
    "    test_image = test_batch[0].to(device)\n",
    "    test_images.append(test_image)\n",
    "    noise_kwargs[\"size\"] = test_image.size()\n",
    "    noise = noise_scale * noise_func(**noise_kwargs).to(device)\n",
    "    noisy_image = test_image + noise\n",
    "    noisy_images.append(noisy_image)\n",
    "    #plt.imshow(test_image.cpu()[0,0])\n",
    "    net.eval()\n",
    "    output = net.forward(noisy_image.float())\n",
    "    outputs.append(output)\n",
    "\n",
    "num_images = len(outputs)\n",
    "fig, ax = plt.subplots(3,num_images)\n",
    "for image_i in range(num_images):\n",
    "    ax[0, image_i].imshow(test_images[image_i].cpu()[0,0])\n",
    "    ax[1, image_i].imshow(noisy_images[image_i].detach().cpu()[0,0])\n",
    "    ax[2, image_i].imshow(outputs[image_i].detach().cpu()[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
