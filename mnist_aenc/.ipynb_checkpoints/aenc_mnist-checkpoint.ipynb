{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sklearn\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1,\"/home/sbulusu/qcd_ml/neural_networks/libs/\")\n",
    "\n",
    "import dataset_class_gsimage\n",
    "import aenc_torch_net_class\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizing CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Utilizing CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizing CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['number']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mnist_image_folder = \"mnist/\"\n",
    "mnist_image_folder = \"../data/mnist/\"\n",
    "mnist_image_names = os.listdir(mnist_image_folder)\n",
    "mnist_image_paths = glob.glob(\"mnist/*\")\n",
    "#mnist_image_paths = []\n",
    "#for mnist_image_name in mnist_image_names:\n",
    "#    mnist_image_paths.append(mnist_image_folder + \"/\" + mnist_image_name)\n",
    "#print(mnist_image_paths)\n",
    "image_size = [1,28,28]\n",
    "label_path_csv = \"../data/mnist_labels.csv\"\n",
    "dataset = dataset_class_gsimage.image_dataset(mnist_image_folder, mnist_image_paths, image_size, label_path_csv)\n",
    "target_attributes = [\"number\"]\n",
    "dataset.set_label_names(target_attributes)\n",
    "#output_attributes = [str(i) for i in range(0,10)]\n",
    "#pred_attributes = [str(i) for i in range(np.prod(image_size))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "plt.imshow(dataset.get_image(i)[0].reshape([28,28]))\n",
    "print(dataset.get_image(i)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "HYPERPARAMETERS\n",
    "\"\"\"\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "val_epochs = [10,20,30,40,50,60,70]\n",
    "#val_epochs = [1,2]\n",
    "save_state_epochs = [10000]\n",
    "\n",
    "\n",
    "\"\"\"Manually set network structure\"\"\"\n",
    "\"\"\"\n",
    "    This list can be loaded into the constructor of the Net neural network class, to automatically generate the network structure\n",
    "    type = pointer to the layer function'\n",
    "    layer_pars = parameters which must be given to the layer function in order to initialize it\n",
    "    act_func = activation function to be applied directly after feeding to the corresponding layer\n",
    "    dropout = certain neurons cna be dropped out if specified\n",
    "\"\"\"\n",
    "\n",
    "#fixed_net_struct = []\n",
    "input_size = dataset.get_input_size()\n",
    "#target_size = len(target_attributes)\n",
    "#output_size = target_size\n",
    "#output_size = len(output_attributes)\n",
    "output_size = input_size\n",
    "\n",
    "\"\"\"\n",
    "#[ [[in_channels, out_channels],[kernel_size], stride, padding], ... ]\n",
    "#kernel_pars = [ [[input_size[0],4],[12,3],1,0], [[4,8],[12,3],1,0], [[8,16],[12,3],1,0], [[16,16],[12,3],1,0], [[16,16],[8,2],1,0] ]\n",
    "kernel_pars = [ [[input_size[0],4],[9,3],1,0], [[4,4],[4,1],[4,1],0], [[4,8],[13,3],1,0], [[8,8],[4,1],[4,1],0], [[8,16],[4,3],1,0]]\n",
    "act_func = torch.relu\n",
    "#, \"act_func\": act_func\n",
    "for i, kernel_par in enumerate(kernel_pars):\n",
    "    if i%2 == 0:\n",
    "        layer_type = nn.Conv2d\n",
    "        fixed_net_struct.append( {\"type\": layer_type, \"layer_pars\": {\"in_channels\": kernel_par[0][0], \"out_channels\": kernel_par[0][1], \"kernel_size\": kernel_par[1], \"stride\": kernel_par[2], \"padding\": kernel_par[3], \"bias\": True}} )\n",
    "    else:\n",
    "        #layer_type = nn.MaxPool2d\n",
    "        layer_type = nn.AvgPool2d\n",
    "        fixed_net_struct.append( {\"type\": layer_type, \"layer_pars\": {\"kernel_size\": kernel_par[1], \"stride\": kernel_par[2], \"padding\": kernel_par[3]}} )\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "for i, kernel_par in enumerate(kernel_pars):\n",
    "        layer_type = nn.Conv2d\n",
    "        fixed_net_struct.append( {\"type\": layer_type, \"layer_pars\": {\"in_channels\": kernel_par[0][0], \"out_channels\": kernel_par[0][1], \"kernel_size\": kernel_par[1], \"stride\": kernel_par[2], \"padding\": kernel_par[3], \"bias\": True}, \"act_func\": act_func} )\n",
    "    \n",
    "\"\"\"\n",
    "\"\"\"\n",
    "conv_sizes = utils.calc_layer_sizes(input_size, fixed_net_struct)\n",
    "fc_input_size = np.product(conv_sizes[-1])\n",
    "\"\"\"\n",
    "\n",
    "encoder_struct = []\n",
    "decoder_struct = []\n",
    "latent_size = 10\n",
    "\n",
    "act_func = torch.relu\n",
    "\n",
    "fc_input_size = np.product(input_size)\n",
    "print(fc_input_size)\n",
    "encoder_struct.append( {\"type\": nn.Flatten, \"layer_pars\": {\"start_dim\": 1}} )\n",
    "encoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": fc_input_size, \"out_features\": fc_input_size//4}, \"bias\": True, \"act_func\": act_func} )\n",
    "encoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": fc_input_size//4, \"out_features\": fc_input_size//16}, \"bias\": True, \"act_func\": act_func} )\n",
    "encoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": fc_input_size//16, \"out_features\": latent_size}, \"bias\": True, \"act_func\": act_func} )\n",
    "\n",
    "decoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": latent_size, \"out_features\": fc_input_size//16}, \"bias\": True, \"act_func\": act_func} )\n",
    "decoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": fc_input_size//16, \"out_features\": fc_input_size//4}, \"bias\": True, \"act_func\": act_func} )\n",
    "decoder_struct.append( {\"type\": nn.Linear, \"layer_pars\": {\"in_features\": fc_input_size//4, \"out_features\": fc_input_size}, \"bias\": True, \"act_func\": act_func} )\n",
    "decoder_struct.append( {\"type\": utils.Reshape, \"layer_pars\": {\"new_shape\": np.concatenate(([-1],input_size))}} )\n",
    "#fixed_net_struct.append( {\"type\": nn.Softmax, \"layer_pars\": {\"dim\": 1}} )\n",
    "#dim 0 or 1???\n",
    "#fixed_net_struct.append( {\"type\": nn.ConvTranspose2d, \"layer_pars\": {\"in_channels\": 1, \"out_channels\": 1, \"kernel_size\": [1,1], \"stride\": 1, \"padding\": 1, \"bias\": True}, \"act_func\": act_func} )\n",
    "\n",
    "#layer_sizes = utils.calc_layer_sizes(input_size, fixed_net_struct)\n",
    "#print(layer_sizes)\n",
    "\n",
    "\"\"\"create list of parameters manually\"\"\"\n",
    "\n",
    "hyper_parameters = {}\n",
    "#hyper_parameters[\"loss_func\"] = nn.CrossEntropyLoss\n",
    "hyper_parameters[\"loss_func\"] = nn.MSELoss\n",
    "hyper_parameters[\"optimizer\"] = optim.Adam\n",
    "hyper_parameters[\"batch_size\"] = 1 \n",
    "hyper_parameters[\"lr\"] = 0.0001\n",
    "#hyper_parameters[\"net_struct\"] = fixed_net_struct\n",
    "\n",
    "hyper_parameters[\"val_method\"] = \"holdout\"\n",
    "hyper_parameters[\"val_method_pars\"] = {\"train\" : 0.9, \"val\" : 0.1, \"test\" : 0.}\n",
    "#hyper_parameters[\"val_method\"] = \"k_fold\"\n",
    "#hyper_parameters[\"val_method_pars\"] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = max(val_epochs)+1\n",
    "\n",
    "lr=hyper_parameters[\"lr\"]\n",
    "batch_size = hyper_parameters[\"batch_size\"]\n",
    "loss_func = hyper_parameters[\"loss_func\"]()\n",
    "#net_struct = hyper_parameters[\"net_struct\"]\n",
    "val_method = hyper_parameters[\"val_method\"]\n",
    "val_method_pars = hyper_parameters[\"val_method_pars\"]\n",
    "optimizer_type = hyper_parameters[\"optimizer\"]\n",
    "\n",
    "val_pred_paths = []\n",
    "val_label_paths = []\n",
    "\n",
    "train_loss = np.zeros(epochs)\n",
    "val_loss = np.zeros(len(val_epochs))\n",
    "\n",
    "net_state_paths = []\n",
    "\n",
    "#create training log\n",
    "log_file_name = \"log.txt\"\n",
    "log_file = open(log_file_name, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Encoder:\n",
      "\n",
      "Adding {'type': <class 'torch.nn.modules.flatten.Flatten'>, 'layer_pars': {'start_dim': 1}}\n",
      "\n",
      "Adding {'type': <class 'torch.nn.modules.linear.Linear'>, 'layer_pars': {'in_features': 784, 'out_features': 196}, 'bias': True, 'act_func': <built-in method relu of type object at 0x7fa05e2ebbe0>}\n",
      "\n",
      "Adding {'type': <class 'torch.nn.modules.linear.Linear'>, 'layer_pars': {'in_features': 196, 'out_features': 49}, 'bias': True, 'act_func': <built-in method relu of type object at 0x7fa05e2ebbe0>}\n",
      "\n",
      "Adding {'type': <class 'torch.nn.modules.linear.Linear'>, 'layer_pars': {'in_features': 49, 'out_features': 10}, 'bias': True, 'act_func': <built-in method relu of type object at 0x7fa05e2ebbe0>}\n",
      "\n",
      "Initializing weights of Encoder with method <function xavier_normal_ at 0x7fa00f754b00>\n",
      "\n",
      "Initializing Decoder:\n",
      "\n",
      "Adding {'type': <class 'torch.nn.modules.linear.Linear'>, 'layer_pars': {'in_features': 10, 'out_features': 49}, 'bias': True, 'act_func': <built-in method relu of type object at 0x7fa05e2ebbe0>}\n",
      "\n",
      "Adding {'type': <class 'torch.nn.modules.linear.Linear'>, 'layer_pars': {'in_features': 49, 'out_features': 196}, 'bias': True, 'act_func': <built-in method relu of type object at 0x7fa05e2ebbe0>}\n",
      "\n",
      "Adding {'type': <class 'torch.nn.modules.linear.Linear'>, 'layer_pars': {'in_features': 196, 'out_features': 784}, 'bias': True, 'act_func': <built-in method relu of type object at 0x7fa05e2ebbe0>}\n",
      "\n",
      "Adding {'type': <class 'utils.Reshape'>, 'layer_pars': {'new_shape': array([-1,  1, 28, 28])}}\n",
      "\n",
      "Initializing weights of Decoder with method <function xavier_normal_ at 0x7fa00f754b00>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tb = SummaryWriter()\n",
    "\n",
    "#net = torch_net_class.Net(net_struct, dataset.get_input_size())    \n",
    "net = aenc_torch_net_class.AutoEncoder(encoder_struct=encoder_struct, decoder_struct=decoder_struct, input_size=input_size, latent_size=latent_size)\n",
    "#net.init_weights(torch.nn.init.xavier_normal_)\n",
    "net.set_batch_size(batch_size)\n",
    "#net.set_layer_sizes(layer_sizes)\n",
    "net.to(device)\n",
    "#net.show_layers()\n",
    "net_parameters = net.parameters()\n",
    "\n",
    "optimizer = optimizer_type(net_parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of val set :1112\n",
      "\n",
      "size of train set :8888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"SPECIFY OUTSIDE FOR K-FOLD VALIDATION\"\"\"\n",
    "data_loader = utils.load_split_data(dataset=dataset, batch_size=batch_size, method=val_method, method_pars=val_method_pars, random_seed=random_seed, log_file=log_file)\n",
    "\n",
    "if val_method == \"holdout\":\n",
    "    data_loader = data_loader[0]\n",
    "    test_loader = data_loader[2]\n",
    "val_loader = data_loader[1]\n",
    "train_loader = data_loader[0]\n",
    "\n",
    "val_dir = \"val/\"\n",
    "try:\n",
    "    os.makedirs(val_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "train_state_dir = \"net_states/\"\n",
    "try:\n",
    "    os.makedirs(train_state_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "epoch = 0\n",
    "val_i = 0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    batch_nr = 0\n",
    "    epoch_loss = np.zeros(len(train_loader))\n",
    "    \n",
    "    \"\"\"Actual training step\"\"\"\n",
    "    for train_mini_batch in train_loader:\n",
    "        batch_loss, train_output = utils.step(net, train_mini_batch, output_size, loss_func, optimizer, device, mode=\"train\", log_file=log_file)\n",
    "        epoch_loss[batch_nr] = batch_loss.item()\n",
    "        batch_nr += 1\n",
    "    mean_epoch_loss = epoch_loss.mean()\n",
    "    train_loss[epoch] = mean_epoch_loss\n",
    "    print(f\"mean epoch {epoch} train loss: {mean_epoch_loss}\\n\")\n",
    "    \n",
    "    \n",
    "    \"\"\"save the neural networks state\"\"\"\n",
    "    if epoch in save_state_epochs:\n",
    "        train_state_epoch_file_path = train_state_dir + f\"state_epoch_{epoch}\"\n",
    "        train_state = {\"epoch\" : epoch, \"state_dict\": net.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "        torch.save(train_state, train_state_epoch_file_path)\n",
    "        net_state_paths.append(train_state_epoch_file_path)\n",
    "        print(f\"saved model from epoch {epoch}\")\n",
    "        \n",
    "    \"\"\"\n",
    "    Validation\n",
    "    \"\"\"\n",
    "    #val_epochs = [100000]\n",
    "    if epoch in val_epochs:\n",
    "        val_label = []\n",
    "        val_pred = []\n",
    "        \n",
    "        val_batch_nr = 0\n",
    "        val_epoch_loss = np.zeros(len(val_loader))\n",
    "        \n",
    "        for val_mini_batch in val_loader:\n",
    "            label_batch = val_mini_batch[1]\n",
    "            val_label.append(label_batch.detach().cpu().numpy())\n",
    "            val_batch_loss, val_output = utils.step(net, val_mini_batch, output_size, loss_func, optimizer, device, mode=\"val\", log_file=log_file)\n",
    "            val_epoch_loss[val_batch_nr] = val_batch_loss.item()\n",
    "            val_pred.append(val_output.detach().cpu().numpy())\n",
    "            \"\"\"\n",
    "            class_batch_pred = []\n",
    "            #print(val_output)\n",
    "            for val in val_output:\n",
    "                class_index = val.argmax().detach().cpu()\n",
    "                class_batch_pred.append(class_index)\n",
    "            #print(class_batch_pred)\n",
    "            val_pred.append(class_batch_pred)\n",
    "            \"\"\"\n",
    "            val_batch_nr += 1\n",
    "            \n",
    "        mean_val_epoch_loss = val_epoch_loss.mean()\n",
    "        val_loss[val_i] = mean_val_epoch_loss\n",
    "        print(f\"mean epoch {epoch} val loss: {mean_val_epoch_loss}\\n\")\n",
    "        \n",
    "        val_i += 1\n",
    "        \n",
    "        val_pred_path = val_dir + \"/\" + f\"val_epoch_{epoch}_pred\"\n",
    "        val_label_path = val_dir + \"/\" + f\"val_epoch_{epoch}_labels\"\n",
    "        #print(np.array(functools.reduce(operator.iconcat, val_pred, [])))\n",
    "        np.array(functools.reduce(operator.iconcat, val_pred, [])).tofile(val_pred_path, sep=\" \")\n",
    "        np.array(functools.reduce(operator.iconcat, val_label, [])).tofile(val_label_path, sep=\" \")\n",
    "        #np.array(val_pred).tofile(val_pred_path)\n",
    "        #np.array(val_label).tofile(val_pred_label)\n",
    "        \n",
    "        val_pred_paths.append(val_pred_path)\n",
    "        val_label_paths.append(val_label_path)\n",
    "        \n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_state_file_path = train_state_dir + f\"state_epoch_{epoch}\"\n",
    "#train_state = {\"epoch\" : epoch, \"state_dict\": net.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "torch.save(net.state_dict(), train_state_file_path)\n",
    "net_state_paths.append(train_state_epoch_file_path)\n",
    "print(f\"saved model from epoch {epoch}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net_state_path = \"net_states/state_epoch_30\"\n",
    "net.load_state_dict(torch.load(net_state_path))\n",
    "#net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image, test_label = dataset[0]\n",
    "indices = np.arange(0,10)\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=test_sampler)\n",
    "test_images = []\n",
    "outputs = []\n",
    "for test_batch in train_loader:\n",
    "    test_image = test_batch[0].to(device)\n",
    "    test_images.append(test_image)\n",
    "    plt.imshow(test_image.cpu()[0,0])\n",
    "    net.eval()\n",
    "    output = net.forward(test_image.float())\n",
    "    outputs.append(output)\n",
    "\n",
    "num_images = len(outputs)\n",
    "fig, ax = plt.subplots(2,num_images)\n",
    "for image_i in range(num_images):\n",
    "    ax[0, image_i].imshow(test_images[image_i].cpu()[0,0])\n",
    "    ax[1, image_i].imshow(outputs[image_i].detach().cpu()[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "latent_samples = torch.randint(low=1, high=1000, size=(num_samples, latent_size) )\n",
    "\n",
    "\"\"\"latent_samples_indices = np.random.randint(low=0, high=9, size=num_samples)\n",
    "latent_samples = np.zeros((num_samples, latent_size))\n",
    "latent_samples[np.arange(num_samples),latent_samples_indices] +=1\n",
    "latent_samples = torch.tensor(latent_samples)\n",
    "print(latent_samples[0])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net.decoder(latent_samples.to(device).float())\n",
    "print(output.size())\n",
    "print(len(output))\n",
    "fig, ax = plt.subplots(1,num_samples)\n",
    "for i in range(num_samples):\n",
    "    ax[i].imshow(output.detach().cpu()[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
